{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning: Project Part 2**\n",
    "\n",
    "---\n",
    "\n",
    "**Author: Damien Farrell**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "load_dotenv()\n",
    "# HF_API_KEY = os.getenv(\"HF_API_KEY\")\n",
    "# assert HF_API_KEY, \"Please set your Hugging Face API key in the HF_API_KEY environment variable.\"\n",
    "\n",
    "# Audio File Path\n",
    "# AUDIO_FILE_PATH = \"./audio/TrumpHarrisDebate.wav\"\n",
    "# assert AUDIO_FILE_PATH, \"Please place your audio in a folder called 'audio'. Note: use backslashes if on Windows\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Part 2: Seal Call Discrimination**\n",
    "\n",
    "### Objective\n",
    "The aim of this project is to analyze a recorded dataset to investigate the feasibility of discriminating between different seal calls. The project is structured in steps to build a machine learning model that can potentially detect seal calls from audio recordings. While creating a fully functional detector is the ultimate goal, it may not be the final outcome of this project.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "#### Step 1: Data Pre-processing and Management\n",
    "1. **Dataset Preparation**:\n",
    "   - Work with provided uncompressed `.wav` files, which may contain large portions without any seal calls.\n",
    "   - Create a dataset from annotated data using the provided Jupyter notebook.\n",
    "   \n",
    "2. **Spectrogram Extraction**:\n",
    "   - Extract spectrograms for each seal call, ensuring consistent size by determining the longest call (in time) and the broadest in frequency.\n",
    "   - Save spectrograms as raw 2D arrays (not images) with metadata.\n",
    "\n",
    "3. **Handling No-Call Data**:\n",
    "   - Create \"no-call\" spectrograms from unannotated regions of recordings, ensuring they match the frequency region of call spectrograms.\n",
    "\n",
    "COMPLETE IN OTHER NOTEBOOK `data_preprocessing.ipynb` !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Model Training\n",
    "1. **Dataset Utilisation**:\n",
    "   - Use the prepared spectrogram dataset to train the machine learning model.\n",
    "\n",
    "3. **Evaluation**:\n",
    "   - Test the modelâ€™s performance on a separate test dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Refine\n",
    "1. **Parameter Tuning**:\n",
    "   - Adjust parameters in the data extraction/spectrogram creation process (e.g., `nfft`, `noverlap`).\n",
    "   - Manage computational costs by splitting `.wav` files using tools like `pydub`.\n",
    "\n",
    "2. **Validation**:\n",
    "   - Apply the Rupe B/No Call classifier on an entire `.wav` file (held back during training).\n",
    "   - Ensure no false positives due to the presence of other annotated calls.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **1. Performs Speaker Diarisation Analysis** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **2. Performs Speech to Text Analysis** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4. Testing & Evaluation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### **References**\n",
    "\n",
    "1. [Bird Song Dataset on Kaggle](https://www.kaggle.com/code/sophiagnetneva/cnn-for-sound-classification-bird-calls-90)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
